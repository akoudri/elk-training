input {
  # Receive data from Filebeat
  beats {
    port => 5044
  }
}

filter {
  # Only process nginx logs
  if [log_type] == "nginx" or "nginx" in [tags] {

    # Parse nginx access logs using grok pattern
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
      add_tag => [ "nginx_access_parsed" ]
      tag_on_failure => [ "_grokparsefailure_nginx" ]
    }

    # If grok was successful, enrich with additional data
    if "_grokparsefailure_nginx" not in [tags] {

      # Parse the timestamp from the log
      date {
        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
        target => "@timestamp"
      }

      # Convert response code to integer
      mutate {
        convert => {
          "response" => "integer"
          "bytes" => "integer"
        }
      }

      # Add GeoIP information based on client IP
      geoip {
        source => "clientip"
        target => "geoip"
        add_tag => [ "geoip" ]
      }

      # Add tag for 404 errors (not found)
      if [response] == 404 {
        mutate {
          add_tag => [ "not_found" ]
          add_field => { "error_type" => "not_found" }
        }
      }

      # Add tag for 5xx errors (server errors)
      if [response] >= 500 {
        mutate {
          add_tag => [ "server_error" ]
          add_field => { "error_type" => "server_error" }
        }
      }

      # Add tag for 4xx errors (client errors)
      if [response] >= 400 and [response] < 500 {
        mutate {
          add_tag => [ "client_error" ]
          add_field => { "error_type" => "client_error" }
        }
      }

      # Add tag for successful responses
      if [response] >= 200 and [response] < 300 {
        mutate {
          add_tag => [ "success" ]
        }
      }

      # Parse the user agent
      useragent {
        source => "agent"
        target => "user_agent"
      }

      # Extract URL path components
      if [request] {
        grok {
          match => { "request" => "%{WORD:http_method} %{URIPATHPARAM:url_path} HTTP/%{NUMBER:http_version}" }
          tag_on_failure => [ "_url_parse_failure" ]
        }
      }

      # Clean up some fields
      mutate {
        remove_field => [ "timestamp" ]
        add_field => { "log_source" => "nginx" }
      }
    }
  }

  # Add common metadata fields
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "nginx-logs"
      "[@metadata][pipeline]" => "nginx-pipeline"
    }
  }
}

output {
  # Output to Elasticsearch with daily indices
  elasticsearch {
    hosts => ["http://es01:9200", "http://es02:9200", "http://es03:9200"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
  }

  # Uncomment for debugging - outputs to console
  # stdout {
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }
}
